{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PointNet++ Custom Dataset Training Notebook\n",
    "# ===========================================\n",
    "\n",
    "########################################\n",
    "# 1. Setup and Imports\n",
    "########################################\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "########################################\n",
    "# Plant-Only to Plant-With-Noise Preprocessing Notebook\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_processing.data_augmentation import add_random_noise_points\n",
    "from src.data_processing.data_preprocessing import (\n",
    "    voxel_down_sample_with_indices,\n",
    "    adjust_point_count_with_indices,\n",
    "    normalize_point_cloud,\n",
    ")\n",
    "from src.utils.data_utils import load_point_cloud  # Ensure this is implemented\n",
    "from src.models.pointnetplusplus import PointNetPlusPlus\n",
    "from src.models.pointnet2_utils import PointNetSetAbstraction, PointNetSetAbstractionMsg, PointNetFeaturePropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################\n",
    "# 1. Setup\n",
    "########################################\n",
    "\n",
    "#project_root = '.'  # Adjust if needed\n",
    "raw_root = os.path.join(project_root, 'data', 'raw')\n",
    "processed_root = os.path.join(project_root, 'data', 'processed')\n",
    "\n",
    "plant_only_dir = os.path.join(raw_root, 'plant_only')\n",
    "output_dir = os.path.join(processed_root, 'plant_only_with_noise')\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "splits_dir = os.path.join(processed_root, 'splits')\n",
    "for d in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(splits_dir, d), exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "num_noise_points = 5000\n",
    "voxel_size = 0.005  # Adjust as needed\n",
    "num_points = 2048\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1  # train+val+test=1.0\n",
    "\n",
    "# Noise parameters for add_random_noise_points\n",
    "noise_params = {\n",
    "    'num_noise_points': num_noise_points,\n",
    "    'color_options': [\n",
    "        (0.0, 0.0, 1.0),  # Blue\n",
    "        (0.0, 0.0, 0.0),  # Black\n",
    "        (1.0, 1.0, 1.0)   # White\n",
    "    ],\n",
    "    'extend_ratio': 0.1,\n",
    "    'noise_below_ratio': 0.7\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2. List all plant-only .ply files\n",
    "########################################\n",
    "\n",
    "def load_ply_files(directory):\n",
    "    files = []\n",
    "    for root, dirs, fs in os.walk(directory):\n",
    "        for f in fs:\n",
    "            if f.endswith('.ply'):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return sorted(files)\n",
    "\n",
    "plant_files = load_ply_files(plant_only_dir)\n",
    "print(f\"Found {len(plant_files)} plant-only .ply files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3. Process Each File: Add noise, downsample, adjust count, normalize\n",
    "########################################\n",
    "\n",
    "# Function to process a single .ply file and save .npz\n",
    "def process_file(ply_path, output_dir, num_points, voxel_size, noise_params):\n",
    "    # Load original plant-only point cloud\n",
    "    pcd = o3d.io.read_point_cloud(ply_path)\n",
    "    points = np.asarray(pcd.points, dtype=np.float32)\n",
    "    colors = np.asarray(pcd.colors, dtype=np.float32) if pcd.has_colors() else None\n",
    "\n",
    "    # Add noise points: returns combined_points, combined_colors, labels (0 or 1)\n",
    "    combined_points, combined_colors, labels = add_random_noise_points(points, colors, **noise_params)\n",
    "    print(\"After noise addition:\")\n",
    "    print(\"Unique labels:\", np.unique(labels))\n",
    "    print(\"Plant points:\", np.sum(labels==1))\n",
    "    print(\"Noise points:\", np.sum(labels==0))\n",
    "\n",
    "    # Convert to Open3D pcd\n",
    "    noisy_pcd = o3d.geometry.PointCloud()\n",
    "    noisy_pcd.points = o3d.utility.Vector3dVector(combined_points)\n",
    "    noisy_pcd.colors = o3d.utility.Vector3dVector(combined_colors)\n",
    "\n",
    "    # Voxel downsample\n",
    "    downsampled_pcd, downsampled_indices = voxel_down_sample_with_indices(noisy_pcd, voxel_size)\n",
    "    downsampled_labels = labels[downsampled_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"After voxel downsample:\")\n",
    "    print(\"Unique labels:\", np.unique(downsampled_labels))\n",
    "    print(\"Plant points:\", np.sum(downsampled_labels==1))\n",
    "    print(\"Noise points:\", np.sum(downsampled_labels==0))\n",
    "\n",
    "    # Adjust point count\n",
    "    adjusted_pcd, adjusted_labels = adjust_point_count_with_indices(downsampled_pcd, downsampled_labels, num_points)\n",
    "    adjusted_points = np.asarray(adjusted_pcd.points, dtype=np.float32)\n",
    "    adjusted_colors = np.asarray(adjusted_pcd.colors, dtype=np.float32)\n",
    "    #adjusted_labels = downsampled_labels[adjusted_indices]\n",
    "\n",
    "\n",
    "    print(\"After adjusting point count:\")\n",
    "    print(\"Unique labels:\", np.unique(adjusted_labels))\n",
    "    print(\"Plant points:\", np.sum(adjusted_labels==1))\n",
    "    print(\"Noise points:\", np.sum(adjusted_labels==0))\n",
    "\n",
    "    # Normalize\n",
    "    normalized_pcd = normalize_point_cloud(adjusted_pcd)\n",
    "    final_points = np.asarray(normalized_pcd.points, dtype=np.float32)\n",
    "\n",
    "    # Save to npz\n",
    "    base_name = os.path.splitext(os.path.basename(ply_path))[0]\n",
    "    out_path = os.path.join(output_dir, base_name + '.npz')\n",
    "    np.savez(out_path, points=final_points, labels=adjusted_labels)\n",
    "    return out_path\n",
    "\n",
    "processed_files = []\n",
    "for ply_file in plant_files:\n",
    "    out_path = process_file(ply_file, output_dir, num_points, voxel_size, noise_params)\n",
    "    processed_files.append(out_path)\n",
    "\n",
    "print(f\"Processed {len(processed_files)} files into {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 4. Splitting into Train/Val/Test\n",
    "########################################\n",
    "\n",
    "# We have processed_files list of npz\n",
    "# We want to split into train/val/test by given ratios\n",
    "\n",
    "train_files, temp_files = train_test_split(processed_files, train_size=train_ratio, random_state=42)\n",
    "val_size = val_ratio / (val_ratio + test_ratio)\n",
    "val_files, test_files = train_test_split(temp_files, train_size=val_size, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "def copy_files(file_list, target_dir):\n",
    "    for f in file_list:\n",
    "        shutil.copy(f, target_dir)\n",
    "\n",
    "copy_files(train_files, os.path.join(splits_dir, 'train'))\n",
    "copy_files(val_files, os.path.join(splits_dir, 'val'))\n",
    "copy_files(test_files, os.path.join(splits_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 5. Verification\n",
    "########################################\n",
    "\n",
    "# Let's load one example from train and print stats\n",
    "sample_file = train_files[0]\n",
    "data = np.load(sample_file)\n",
    "points = data['points']\n",
    "labels = data['labels']\n",
    "print(\"Verification of a sample:\")\n",
    "print(\"Points shape:\", points.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Unique labels:\", np.unique(labels))\n",
    "print(\"Number plant points:\", np.sum(labels==1))\n",
    "print(\"Number noise points:\", np.sum(labels==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_npz_file(npz_path, title='Point Cloud'):\n",
    "    \"\"\"\n",
    "    Visualizes a point cloud from an .npz file with points colored based on labels.\n",
    "\n",
    "    Args:\n",
    "        npz_path (str): Path to the .npz file.\n",
    "        title (str): Title for the visualization window.\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    points = data['points']  # (N, 3)\n",
    "    labels = data['labels']  # (N,)\n",
    "\n",
    "    # Assign colors based on labels\n",
    "    colors = np.zeros((points.shape[0], 3))\n",
    "    colors[labels == 1] = [0, 1, 0]  # Green for plant\n",
    "    colors[labels == 0] = [1, 0, 0]  # Red for noise\n",
    "\n",
    "    # Create Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=title)\n",
    "\n",
    "# Example usage:\n",
    "sample_npz = os.path.join(output_dir, 'Wheat_Gladius_B6_2023-06-27-2029_fused_output.npz')  # Replace with your sample file\n",
    "visualize_npz_file(sample_npz, title='Sample Plant-With-Noise Point Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 2  # 0=noise, 1=plant\n",
    "model = PointNetPlusPlus(num_classes=num_classes).to(device)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create Custom PyTorch Dataset\n",
    "\n",
    "class ProcessedPointCloudDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by listing all .npz files in the directory.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Directory containing .npz files.\n",
    "        \"\"\"\n",
    "        self.files = []\n",
    "        for root, dirs, fs in os.walk(directory):\n",
    "            for fname in fs:\n",
    "                if fname.endswith('.npz'):\n",
    "                    self.files.append(os.path.join(root, fname))\n",
    "        self.files = sorted(self.files)\n",
    "        print(f\"Dataset initialized with {len(self.files)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the points and labels from a single .npz file.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the file to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Points tensor and labels tensor.\n",
    "        \"\"\"\n",
    "        data = np.load(self.files[idx])\n",
    "        points = data['points']  # (N, 3)\n",
    "        labels = data['labels']  # (N,)\n",
    "        return points, labels\n",
    "\n",
    "\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = ProcessedPointCloudDataset(os.path.join(splits_dir, 'train'))\n",
    "val_dataset = ProcessedPointCloudDataset(os.path.join(splits_dir, 'val'))\n",
    "test_dataset = ProcessedPointCloudDataset(os.path.join(splits_dir, 'test'))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for points, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        # Move data to device\n",
    "        points = points.float().to(device)  # (B, N, 3)\n",
    "        labels = labels.long().to(device)   # (B, N)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(points)              # (B, N, num_classes)\n",
    "        outputs = F.log_softmax(outputs, dim=-1)  # Apply log_softmax\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        outputs = outputs.view(-1, num_classes)  # (B*N, num_classes)\n",
    "        labels = labels.view(-1)                 # (B*N,)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    average_loss = running_loss / len(loader)\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_points = 0\n",
    "    with torch.no_grad():\n",
    "        for points, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            # Move data to device\n",
    "            points = points.float().to(device)  # (B, N, 3)\n",
    "            labels = labels.long().to(device)   # (B, N)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(points)              # (B, N, num_classes)\n",
    "            outputs = F.log_softmax(outputs, dim=-1)  # Apply log_softmax\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.view(-1, num_classes)  # (B*N, num_classes)\n",
    "            labels = labels.view(-1)                 # (B*N,)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(outputs, dim=1)     # (B*N,)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_points += labels.numel()\n",
    "            \n",
    "    average_loss = running_loss / len(loader)\n",
    "    accuracy = 100.0 * total_correct / total_points\n",
    "    return average_loss, accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
