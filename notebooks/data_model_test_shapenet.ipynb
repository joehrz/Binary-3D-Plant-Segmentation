{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import standard libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Open3D for visualization (optional)\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "\n",
    "#from src.models.pointnetplusplus import PointNetPlusPlus\n",
    "from src.models.pointnet2_utils import PointNetSetAbstraction, PointNetSetAbstractionMsg, PointNetFeaturePropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapenet_dataset.py\n",
    "\n",
    "class ShapeNetPartDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', num_points=2048, class_choice=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.class_choice = class_choice\n",
    "\n",
    "        # Load data\n",
    "        self.datapath = []\n",
    "        self.classes = {}\n",
    "        self.class_to_seg_map = {}\n",
    "        self.num_seg_classes = 0\n",
    "\n",
    "        # Load the class names\n",
    "        with open(os.path.join(self.root_dir, 'synsetoffset2category.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                cls, idx = line.strip().split()\n",
    "                self.classes[cls] = idx\n",
    "\n",
    "        if self.class_choice:\n",
    "            self.classes = {k: v for k, v in self.classes.items() if k in self.class_choice}\n",
    "\n",
    "        # Load segmentation label mappings\n",
    "        with open(os.path.join(self.root_dir, 'misc', 'num_seg_classes.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                cls, num = line.strip().split()\n",
    "                self.class_to_seg_map[cls] = int(num)\n",
    "                self.num_seg_classes += int(num)\n",
    "\n",
    "        # Load file paths\n",
    "        for cls in self.classes:\n",
    "            cls_root = os.path.join(self.root_dir, self.classes[cls], 'points')\n",
    "            seg_root = os.path.join(self.root_dir, self.classes[cls], 'points_label')\n",
    "\n",
    "            files = sorted(os.listdir(cls_root))\n",
    "            if split == 'train':\n",
    "                files = files[:int(len(files) * 0.9)]\n",
    "            else:\n",
    "                files = files[int(len(files) * 0.9):]\n",
    "\n",
    "            for file in files:\n",
    "                point_file = os.path.join(cls_root, file)\n",
    "                seg_file = os.path.join(seg_root, file.replace('.pts', '.seg'))\n",
    "                self.datapath.append((point_file, seg_file, cls))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_file, seg_file, cls = self.datapath[idx]\n",
    "        point_set = np.loadtxt(point_file).astype(np.float32)\n",
    "        seg = np.loadtxt(seg_file).astype(np.int64) - 1  # Labels start from 1\n",
    "\n",
    "        # Sample points\n",
    "        choice = np.random.choice(len(seg), self.num_points, replace=True)\n",
    "        point_set = point_set[choice, :]\n",
    "        seg = seg[choice]\n",
    "\n",
    "        # Normalize\n",
    "        point_set = point_set - np.mean(point_set, axis=0)\n",
    "        norm = np.max(np.linalg.norm(point_set, axis=1))\n",
    "        point_set = point_set / norm\n",
    "\n",
    "        return point_set, seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "root_dir = './data/shapenet_part/'\n",
    "num_points = 2048\n",
    "batch_size = 16\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ShapeNetPartDataset(root_dir=root_dir, split='train', num_points=num_points)\n",
    "val_dataset = ShapeNetPartDataset(root_dir=root_dir, split='test', num_points=num_points)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Set Abstraction layers with MSG\n",
    "        self.sa1 = PointNetSetAbstractionMsg(\n",
    "            npoint=512,\n",
    "            radii=[0.1, 0.2],\n",
    "            nsamples=[32, 64],\n",
    "            mlps=[[3, 32, 32, 64], [3, 64, 64, 128]]\n",
    "        )\n",
    "        self.sa2 = PointNetSetAbstractionMsg(\n",
    "            npoint=128,\n",
    "            radii=[0.2, 0.4],\n",
    "            nsamples=[64, 128],\n",
    "            mlps=[[195, 128, 128, 256], [195, 256, 256, 512]]\n",
    "        )\n",
    "        self.sa3 = PointNetSetAbstraction(\n",
    "            npoint=None,\n",
    "            radius=None,\n",
    "            nsample=None,\n",
    "            mlp=[771, 512, 1024],\n",
    "            group_all=True\n",
    "        )\n",
    "\n",
    "        # Feature Propagation layers\n",
    "        self.fp3 = PointNetFeaturePropagation(in_channel=1792, mlp=[512, 512])\n",
    "        self.fp2 = PointNetFeaturePropagation(in_channel=704, mlp=[512, 256])\n",
    "        self.fp1 = PointNetFeaturePropagation(in_channel=259, mlp=[256, 128])  # Adjusted to 259\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, xyz: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            xyz (torch.Tensor): Input point cloud data of shape (B, N, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Segmentation scores for each point.\n",
    "        \"\"\"\n",
    "        B, N, _ = xyz.shape\n",
    "\n",
    "        # Input Transformation\n",
    "        l0_xyz = xyz.transpose(2, 1).contiguous()  # Shape: (B, 3, N)\n",
    "        l0_points = None  # No additional features at input\n",
    "\n",
    "        # Set Abstraction layers\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)     # l1_points: 192 channels\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)     # l2_points: 768 channels\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)     # l3_points: 1024 channels\n",
    "\n",
    "        # Feature Propagation layers\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)  # l2_points: 512 channels\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)  # l1_points: 256 channels\n",
    "\n",
    "        # Concatenate l0_xyz with l0_points if l0_points is not None\n",
    "        if l0_points is not None:\n",
    "            in_channel = l0_points.shape[1] + 256  # Adjusted\n",
    "        else:\n",
    "            in_channel = 3 + 256  # XYZ coordinates + features from l1_points\n",
    "\n",
    "        self.fp1 = PointNetFeaturePropagation(in_channel=in_channel, mlp=[256, 128])\n",
    "\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, l0_points, l1_points)  # l0_points: 128 channels\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn1(self.conv1(l0_points)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x.transpose(2, 1).contiguous()  # Shape: (B, N, num_classes)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
