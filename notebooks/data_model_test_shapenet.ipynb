{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import standard libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Open3D for visualization (optional)\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "\n",
    "#from src.models.pointnetplusplus import PointNetPlusPlus\n",
    "from src.models.pointnet2_utils import PointNetSetAbstraction, PointNetSetAbstractionMsg, PointNetFeaturePropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapenet_dataset.py\n",
    "\n",
    "class ShapeNetPartDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', num_points=2048, class_choice=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.class_choice = class_choice\n",
    "\n",
    "        # Load data\n",
    "        self.datapath = []\n",
    "        self.classes = {}\n",
    "        self.class_to_seg_map = {}\n",
    "        self.num_seg_classes = 0\n",
    "\n",
    "        # Load the class names\n",
    "        with open(os.path.join(self.root_dir, 'synsetoffset2category.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                cls, idx = line.strip().split()\n",
    "                self.classes[cls] = idx\n",
    "\n",
    "        if self.class_choice:\n",
    "            self.classes = {k: v for k, v in self.classes.items() if k in self.class_choice}\n",
    "\n",
    "        # Load segmentation label mappings\n",
    "        with open(os.path.join(self.root_dir, 'misc', 'num_seg_classes.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                cls, num = line.strip().split()\n",
    "                self.class_to_seg_map[cls] = int(num)\n",
    "                self.num_seg_classes += int(num)\n",
    "\n",
    "        # Load file paths\n",
    "        for cls in self.classes:\n",
    "            cls_root = os.path.join(self.root_dir, self.classes[cls], 'points')\n",
    "            seg_root = os.path.join(self.root_dir, self.classes[cls], 'points_label')\n",
    "\n",
    "            files = sorted(os.listdir(cls_root))\n",
    "            if split == 'train':\n",
    "                files = files[:int(len(files) * 0.9)]\n",
    "            else:\n",
    "                files = files[int(len(files) * 0.9):]\n",
    "\n",
    "            for file in files:\n",
    "                point_file = os.path.join(cls_root, file)\n",
    "                seg_file = os.path.join(seg_root, file.replace('.pts', '.seg'))\n",
    "                self.datapath.append((point_file, seg_file, cls))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_file, seg_file, cls = self.datapath[idx]\n",
    "        point_set = np.loadtxt(point_file).astype(np.float32)\n",
    "        seg = np.loadtxt(seg_file).astype(np.int64) - 1  # Labels start from 1\n",
    "\n",
    "        # Sample points\n",
    "        choice = np.random.choice(len(seg), self.num_points, replace=True)\n",
    "        point_set = point_set[choice, :]\n",
    "        seg = seg[choice]\n",
    "\n",
    "        # Normalize\n",
    "        point_set = point_set - np.mean(point_set, axis=0)\n",
    "        norm = np.max(np.linalg.norm(point_set, axis=1))\n",
    "        point_set = point_set / norm\n",
    "\n",
    "        return point_set, seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\joe_h\\\\Desktop\\\\work_projects\\\\Plant-Point-Cloud-Filtering-with-Supervised-Segmentation\\\\data\\\\shapenet_part'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '/data/shapenet_part'\n",
    "root_dir = project_root + '\\data\\shapenet_part'\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\joe_h\\\\Desktop\\\\work_projects\\\\Plant-Point-Cloud-Filtering-with-Supervised-Segmentation\\\\data\\\\shapenet_part\\\\misc\\\\num_seg_classes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mShapeNetPartDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ShapeNetPartDataset(root_dir\u001b[38;5;241m=\u001b[39mroot_dir, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, num_points\u001b[38;5;241m=\u001b[39mnum_points)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mShapeNetPartDataset.__init__\u001b[1;34m(self, root_dir, split, num_points, class_choice)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_choice}\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load segmentation label mappings\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmisc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_seg_classes.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mcls\u001b[39m, num \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\joe_h\\\\Desktop\\\\work_projects\\\\Plant-Point-Cloud-Filtering-with-Supervised-Segmentation\\\\data\\\\shapenet_part\\\\misc\\\\num_seg_classes.txt'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "root_dir = project_root + '\\data\\shapenet_part'\n",
    "root_dir\n",
    "\n",
    "num_points = 2048\n",
    "batch_size = 16\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ShapeNetPartDataset(root_dir=root_dir, split='train', num_points=num_points)\n",
    "val_dataset = ShapeNetPartDataset(root_dir=root_dir, split='test', num_points=num_points)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Set Abstraction layers with MSG\n",
    "        self.sa1 = PointNetSetAbstractionMsg(\n",
    "            npoint=512,\n",
    "            radii=[0.1, 0.2],\n",
    "            nsamples=[32, 64],\n",
    "            mlps=[[3, 32, 32, 64], [3, 64, 64, 128]]\n",
    "        )\n",
    "        self.sa2 = PointNetSetAbstractionMsg(\n",
    "            npoint=128,\n",
    "            radii=[0.2, 0.4],\n",
    "            nsamples=[64, 128],\n",
    "            mlps=[[195, 128, 128, 256], [195, 256, 256, 512]]\n",
    "        )\n",
    "        self.sa3 = PointNetSetAbstraction(\n",
    "            npoint=None,\n",
    "            radius=None,\n",
    "            nsample=None,\n",
    "            mlp=[771, 512, 1024],\n",
    "            group_all=True\n",
    "        )\n",
    "\n",
    "        # Feature Propagation layers\n",
    "        self.fp3 = PointNetFeaturePropagation(in_channel=1792, mlp=[512, 512])\n",
    "        self.fp2 = PointNetFeaturePropagation(in_channel=704, mlp=[512, 256])\n",
    "        self.fp1 = PointNetFeaturePropagation(in_channel=259, mlp=[256, 128])  # Adjusted to 259\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, xyz: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            xyz (torch.Tensor): Input point cloud data of shape (B, N, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Segmentation scores for each point.\n",
    "        \"\"\"\n",
    "        B, N, _ = xyz.shape\n",
    "\n",
    "        # Input Transformation\n",
    "        l0_xyz = xyz.transpose(2, 1).contiguous()  # Shape: (B, 3, N)\n",
    "        l0_points = None  # No additional features at input\n",
    "\n",
    "        # Set Abstraction layers\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)     # l1_points: 192 channels\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)     # l2_points: 768 channels\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)     # l3_points: 1024 channels\n",
    "\n",
    "        # Feature Propagation layers\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)  # l2_points: 512 channels\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)  # l1_points: 256 channels\n",
    "\n",
    "        # Concatenate l0_xyz with l0_points if l0_points is not None\n",
    "        if l0_points is not None:\n",
    "            in_channel = l0_points.shape[1] + 256  # Adjusted\n",
    "        else:\n",
    "            in_channel = 3 + 256  # XYZ coordinates + features from l1_points\n",
    "\n",
    "        self.fp1 = PointNetFeaturePropagation(in_channel=in_channel, mlp=[256, 128])\n",
    "\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, l0_points, l1_points)  # l0_points: 128 channels\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn1(self.conv1(l0_points)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x.transpose(2, 1).contiguous()  # Shape: (B, N, num_classes)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
